# Z-Library Crawler Project

A comprehensive Python-based web scraping tool for searching, extracting, and downloading books from Z-Library (zh.z-lib.fm). This project provides automated book discovery, metadata extraction, download link generation, and file management capabilities.

## üöÄ Overview

This project is designed to automate the process of searching for books on Z-Library, extracting detailed metadata, generating download links, and organizing the results. It features advanced filtering capabilities, year-based traversal, book name matching algorithms, and comprehensive configuration options.

## ‚ú® Key Features

### üîç **Advanced Search & Filtering**
- **Multi-criteria Search**: Search by book name, author, language, file type, publication year, and content type
- **Fuzzy Match Control**: Option to include or exclude fuzzy search matches
- **File Type Filtering**: Support for EPUB, PDF, MOBI, AZW3, TXT, FB2, RTF formats
- **Language Preferences**: Target specific languages (e.g., Chinese, English)
- **Publication Year Filtering**: Search within specific year ranges
- **Content Type Selection**: Filter between books and articles

### üìä **Data Extraction & Management**
- **Comprehensive Metadata**: Extract title, author, language, file size, format, and URLs
- **JSON Output**: Structured data storage with configurable file naming
- **Batch Processing**: Handle multiple search queries and pagination
- **Download Link Generation**: Both synchronous and asynchronous methods
- **Session Management**: Persistent login and cookie handling

### ü§ñ **Automation Features**
- **Year Traversal**: Automatically search across multiple years (2000-2025)
- **Selenium WebDriver**: Headless browser automation with Chrome
- **Rate Limiting**: Configurable delays to avoid detection
- **Retry Logic**: Automatic retry on failed requests
- **Progress Tracking**: Detailed logging and statistics

### üìö **Book Name Matching**
- **RapidFuzz Integration**: Advanced fuzzy string matching
- **Name Extraction**: Extract book names from output JSON files
- **Similarity Scoring**: Intelligent book name comparison
- **Duplicate Detection**: Identify similar or duplicate entries

## üìÅ Project Structure

```
SeekHubProject/
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ requirements.txt                    # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                         # Git ignore patterns
‚îú‚îÄ‚îÄ traversal_year.py                  # Year-based search automation
‚îú‚îÄ‚îÄ unprocessesd_json_generator.py     # Raw search data generator
‚îú‚îÄ‚îÄ download_json_generator.py         # Download link generator
‚îú‚îÄ‚îÄ processesd_json_generator.py       # Processed data generator
‚îú‚îÄ‚îÄ OS_function_tests.py              # System function tests
‚îú‚îÄ‚îÄ output/                           # Generated output files
‚îÇ   ‚îú‚îÄ‚îÄ json/                        # Search result JSON files
‚îÇ   ‚îú‚îÄ‚îÄ auth/                        # Authentication data (cookies)
‚îÇ   ‚îî‚îÄ‚îÄ downloads/                   # Downloaded book files
‚îî‚îÄ‚îÄ zlibraryCrowler/                  # Main crawler package
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ main.py                      # Basic web driver setup
    ‚îú‚îÄ‚îÄ config.py                    # Comprehensive configuration
    ‚îú‚îÄ‚îÄ .env.example                 # Environment variables template
    ‚îú‚îÄ‚îÄ login.py                     # Authentication management
    ‚îú‚îÄ‚îÄ search.py                    # Search functionality
    ‚îú‚îÄ‚îÄ getSearchDownloadLinks.py    # Download link extraction
    ‚îú‚îÄ‚îÄ downloadFiles.py             # File download management
    ‚îú‚îÄ‚îÄ getCookies.py                # Cookie handling
    ‚îú‚îÄ‚îÄ textProcess.py               # Text processing utilities
    ‚îî‚îÄ‚îÄ bookNameMatching/            # Book matching algorithms
        ‚îú‚îÄ‚îÄ ExtractNamesFromOutputJson.py  # Name extraction
        ‚îî‚îÄ‚îÄ rapidFuzzMatching.py           # Fuzzy matching
```

## üõ†Ô∏è Installation & Setup

### Prerequisites
- Python 3.8 or higher
- Chrome browser (for Selenium WebDriver)
- Z-Library account credentials

### 1. Clone the Repository
```bash
git clone <repository-url>
cd SeekHubProject
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Environment Configuration
```bash
# Copy the environment template
cp zlibraryCrowler/.env.example zlibraryCrowler/.env

# Edit the .env file with your credentials
EMAIL=your_email@example.com
PASSWORD=your_password
```

### 4. Configure Search Parameters
Edit `zlibraryCrowler/config.py` to set your preferences:

```python
# Basic search configuration
BOOK_NAME_TO_SEARCH = "Python Programming"
PREFERRED_LANGUAGE = "english"
PREFERRED_FILE_TYPES = ["PDF", "EPUB"]
PREFERRED_YEAR = 2020
MAX_PAGES_TO_SCRAPE = 5

# Advanced options
INCLUDE_FUZZY_MATCHES = False
USE_HEADLESS_BROWSER = True
MAX_CONCURRENT_REQUESTS = 3
```

## üöÄ Usage

### Basic Search
```bash
# Generate unprocessed search results
python unprocessesd_json_generator.py

# Extract download links
python download_json_generator.py

# Process and clean data
python processesd_json_generator.py
```

### Year Traversal (Automated)
```bash
# Search across all years from 2000-2025
python traversal_year.py
```

### Book Name Matching
```bash
# Extract book names from JSON files
python zlibraryCrowler/bookNameMatching/ExtractNamesFromOutputJson.py

# Perform fuzzy matching
python zlibraryCrowler/bookNameMatching/rapidFuzzMatching.py
```

## ‚öôÔ∏è Configuration Options

### Search Parameters
| Parameter | Description | Options |
|-----------|-------------|---------|
| `BOOK_NAME_TO_SEARCH` | Target book name | String or None |
| `PREFERRED_LANGUAGE` | Language filter | "chinese", "english", etc. |
| `PREFERRED_FILE_TYPES` | File format filters | ["EPUB", "PDF", "MOBI", "AZW3", "TXT", "FB2", "RTF"] |
| `PREFERRED_YEAR` | Publication year | Integer (0 to ignore) |
| `PREFERRED_CONTENT_TYPES` | Content type filter | ["book", "article"] |
| `PREFERRED_ORDER` | Result ordering | "popular", "bestmatch", "newest", "oldest" |
| `MAX_PAGES_TO_SCRAPE` | Maximum pages to process | Integer |
| `INCLUDE_FUZZY_MATCHES` | Include fuzzy matches | Boolean |

### Performance Settings
| Parameter | Description | Default |
|-----------|-------------|---------|
| `USE_HEADLESS_BROWSER` | Run browser in background | True |
| `MAX_CONCURRENT_REQUESTS` | Async request limit | 3 |
| `REQUEST_DELAY` | Delay between requests (seconds) | 1 |
| `MAX_RETRIES` | Maximum retry attempts | 5 |
| `BROWSER_TIMEOUT` | WebDriver timeout (seconds) | 10 |

### Output Configuration
| Parameter | Description | Default |
|-----------|-------------|---------|
| `OUTPUT_FOLDERS['json']` | JSON output directory | `./output/json/` |
| `OUTPUT_FOLDERS['auth']` | Authentication data directory | `./output/auth/` |
| `OUTPUT_FOLDERS['downloads']` | Downloaded files directory | `./output/downloads/` |
| `PROCESS_NAME` | File naming prefix | "zlibrary_crawler" |

## üìä Output Files

### JSON Structure
```json
{
  "id": "book_unique_id",
  "title": "Book Title",
  "author": "Author Name",
  "language": "english",
  "file_type": "PDF",
  "file_size": "2.5 MB",
  "book_page_url": "https://zh.z-lib.fm/book/...",
  "download_url": "https://zh.z-lib.fm/dl/...",
  "download_links": [...]
}
```

### File Naming Convention
- **Search Results**: `{process_name}_{book_name}_{language}_{file_types}_{year}_{hash}_books.json`
- **Download Links**: `{process_name}_{book_name}_{language}_{file_types}_{year}_{hash}_download_links.json`
- **Downloaded Files**: `{process_name}_{original_filename}.{extension}`

## üîß Advanced Features

### Async Download Link Extraction
The project supports both synchronous (Selenium) and asynchronous (aiohttp) methods for extracting download links:

```python
# Enable async extraction in config.py
USE_ASYNC_EXTRACTION = True
MAX_CONCURRENT_REQUESTS = 3
```

### Rate Limiting & Bot Detection Avoidance
- Configurable delays between requests
- Browser automation controls
- User-agent rotation
- Session persistence

### Error Handling & Logging
- Comprehensive error logging
- Retry mechanisms
- Progress tracking
- Statistics reporting

### Book Name Matching Algorithms
- **RapidFuzz**: Advanced fuzzy string matching
- **Similarity Scoring**: Intelligent comparison metrics
- **Batch Processing**: Handle multiple book comparisons

## üõ°Ô∏è Security & Best Practices

### Authentication
- Store credentials in `.env` file (never commit to version control)
- Session cookies are automatically managed and persisted
- Login status verification on each page navigation

### Rate Limiting
```python
# Recommended settings to avoid being blocked
REQUEST_DELAY = 1          # 1 second between requests
PAGE_LOAD_DELAY = 2        # 2 seconds after page loads
MAX_CONCURRENT_REQUESTS = 3 # Maximum simultaneous requests
```

### Legal Considerations
- This tool is for educational and research purposes
- Respect Z-Library's terms of service
- Use reasonable request rates to avoid server overload
- Ensure you have proper rights to download content

## üö® Troubleshooting

### Common Issues

#### 1. Login Failures
```python
# Check credentials in .env file
EMAIL=your_correct_email@domain.com
PASSWORD=your_correct_password

# Verify Z-Library website availability
ZLIBRARY_BASE_URL = "https://zh.z-lib.fm"
```

#### 2. WebDriver Issues
```bash
# Update Chrome WebDriver
pip install --upgrade webdriver-manager

# Verify Chrome browser installation
google-chrome --version  # Linux
# or check Chrome installation on Windows/Mac
```

#### 3. Rate Limiting
```python
# Increase delays in config.py
REQUEST_DELAY = 2
PAGE_LOAD_DELAY = 3
MAX_CONCURRENT_REQUESTS = 2
```

#### 4. Memory Issues with Large Datasets
```python
# Reduce concurrent operations
MAX_CONCURRENT_REQUESTS = 1
MAX_PAGES_TO_SCRAPE = 3

# Process in smaller batches
# Use year traversal for systematic processing
```

## üìà Performance Optimization

### For Large-Scale Operations
1. **Use Year Traversal**: Process data year by year to manage memory
2. **Enable Async Processing**: Use `USE_ASYNC_EXTRACTION = True`
3. **Optimize Concurrency**: Balance `MAX_CONCURRENT_REQUESTS` vs. rate limits
4. **Monitor Output Sizes**: Large JSON files may need processing in chunks

### Memory Management
```python
# Recommended settings for large datasets
MAX_PAGES_TO_SCRAPE = 5     # Limit pages per search
MAX_CONCURRENT_REQUESTS = 2  # Reduce concurrent operations
USE_HEADLESS_BROWSER = True  # Save memory
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is for educational and research purposes. Please ensure compliance with Z-Library's terms of service and applicable copyright laws.

## üÜò Support

For issues, questions, or contributions:
1. Check the troubleshooting section above
2. Review the configuration options
3. Create an issue with detailed error logs
4. Include your environment details (Python version, OS, etc.)

---

**‚ö†Ô∏è Disclaimer**: This tool is intended for educational and research purposes. Users are responsible for ensuring compliance with all applicable laws and terms of service. The developers are not responsible for any misuse of this software.
